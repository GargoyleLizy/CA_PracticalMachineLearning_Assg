---
title: "HumanActivityRecognition_pml_assignment"
author: "Lizy"
date: "13 August 2016"
output: html_document
---



# Executive Summary
This assignment analysised the human activity recognition data (http://groupware.les.inf.puc-rio.br/har). Several machine learning methods are applied and feature selections are done mannually to improve the performance. 
The final machine learning model generated by Random forest and the performance is promising.

# Getting Data and Processing Data
Since there are columns of the testing data are all NAs, there is no meaning for taking consideration of these attributes in the training part. Therefore we take these values off. 


```r
library(caret)

pml_testing <- read.csv("./pml-testing.csv")
pml_training <- read.csv("./pml-training.csv")

# find the columns that only contains NAs
allmisscols <- apply(pml_testing,2, function(x) all(is.na(x)))
# get the less
less_training <- pml_training[,!allmisscols]


# get training data and validating data set.
set.seed(2333)
half_cols <- sample(1:19622,19622*.5)
half_less_training <- less_training[half_cols, ]
half_less_validating <- less_training[-half_cols,]

# prepare a function for validating.
pml_performance <- function(model, pml_validating){
    this_pred <- predict(model, pml_validating)
    this_tb <- table(this_pred,pml_validating$classe)
    print(this_tb)
    confusionMatrix(this_tb)$overall[1]
}
```

# Training the models
For classification, trained the model with several methods. After testing various methods (As shown in below), the promising methods are "naive bayes" and "Least Squares Support Vector Machine with Radial Basis Function Kernel". The random forest also seems promising based on its performance on the validating set. But it only predicts A on the test set. Which seems implying that the models are overfitting to the training data set.

(All the code of the block below are commented because they would take a lot of time to processing for generating documents. Readers can try these code by uncomment them.)

```r
# Bad models 
# all A, validating shows an accuracy at .5
#lessModElm <- train(classe~., method= 'elm', data = less_training)
# this one failed totally in validating
#lessModMultinom <- train(classe~., method= 'multinom', data = less_training)
# this model only predict A in validation. 
#lessModDnn <- train(classe~., method= 'dnn', data = less_training)

# not so bad methods.
# predict all As again, validating shows an accuracy at 0.9
#lessModAdaBag <- train(classe~., method = "AdaBag", data = less_training)
# validating performance at 0.8
#lessModLSSVM <- train(classe~., method = "lssvmLinear", data = less_training)

# all the three methods above only gives all A prediction. 
# on the other hand, they all passed the validating set with accuracy at 1. 
#lessModGBM <- train(classe~., method = "gbm", data = less_training)

##half_lessModRF <- train(classe~., method = "rf", data=half_less_training, preProcess = "nzv")

#lessModLDA <- train(classe~., method = "lda", data = less_training)

# Models that make some sensable prediciton over the test set.
# validating at accuracy at 0.9
#lessModLSS2 <- train(classe~., method = "lssvmRadial" , data = less_training)

# validating at .9
##half_lessModNB <- train(classe~.,  method = "nb", data = half_less_training)
```

# Feature Selection
According to the information collected, "Random Forest" seems resistent to "overfitting". Then I realize that noise is a highly possible reason for the bad performance in test set. If the random forest used too many noise features, it can be biased for these noise features. Hence I checked the features again and several features seems to be noise. They are "X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", and "cvtd_timestamp", respectively. Then these features are removed and a new Random Forest model was built based on the filtered data set.
The performance of the new model is pretty good. 


```r
# remove columns that shall be meaningless to the learning models.
half_manul_pick_training <- subset(half_less_training, select = -c(X,user_name,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp))

# Train the random forest on the filtered data set.
half_filtedModRF <- train(classe~., method = "rf", data=half_manul_pick_training)

# performance
pml_performance(half_filtedModRF, half_less_validating)
```

```
##          
## this_pred    A    B    C    D    E
##         A 2758    3    0    0    0
##         B    0 1883    8    0    1
##         C    0    1 1728    8    0
##         D    0    1    0 1615    8
##         E    1    0    0    3 1793
```

```
##  Accuracy 
## 0.9965345
```

# Final Prediction
As shown above, the random forest have a pretty good performance - 0.996 - in the validating set. 
So the final prediction for the pml-testing are shown below. 
(These predicitons are all correct based on the quiz.)


```r
# The final predicitons
predict(half_filtedModRF, pml_testing)
```

```
##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E
```

